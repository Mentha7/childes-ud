\chapter{Implementation of chatconllu} % Main chapter title

\label{Chapter3} % For referencing the chapter elsewhere, use \ref{Chapter3}

To make the tool manageable within the frame of a bachelor's thesis, chatconllu is designed based on several selected corpora from the CHILDES database.

\section{General Structure of the Program}
\section{\emph{CHAT} to \emph{CoNLL-U}}
\section{\emph{CoNLL-U} to \emph{CHAT}}
%\section{Organization of Outputs}


%\section{Processing \emph{CHAT} Files}
%\subsubsection{Normalising Utterances}
%
%\subsubsection{Extracting Information}
%\subsubsection{MOR}
%\subsubsection{GRA}

\section{Normalising Utterances}
\subsection{Challenge: Nested Angel Brackets}

The original algorithm was successful in processing flat structure with CHAT codes, but not nested structures.

Nested structures occurs with scoped symbols. Scoped symbols refer to a segment of speech instead of a single word. The tokens/words in the segment are enclosed in angle brackets, while the markers which describe the segment are enclosed in square brackets that follows the angle brackets. There can be more than one square-bracket-enclosed descriptors following one speech segment, but no other materials should be allowed between the scope enclosed in angle brackets and the symbols enclosed in square brackets.\\
The challenge of nested angle brackets can be illustrated using the following examples.\\
% example from Brown/Sarah/21024, sentence 733

\texttt{<<yeah yeah> [/] yeah> [?] .}

Although chatconllu can clean the CHAT annotation codes relatively well, it remains a question whether one really wants to obtain the clean version of the utterance. On the one hand, the word-based information on \texttt{MOR} and \texttt{GRA} tiers are only associated with the clean tokens, so to use the morphosyntactic information, one has to remove unannotated words (or nonwords). On the other hand, removing the speech errors make spontaneous speech less speech-like. Worse, it could make the sentence structure less obvious, at least for the dependency parsers. As (\cite{odijk2018anncor}) observed in cleaning the CHAT utterances, blindly removing unintelligible words results in a decrease in dependency parser performance. For example, although \texttt{xxx} is a nonword, it still functions as a placeholder \sidenote{Think of it as 0 in digits or as a masked token.} and the Alpino parser seems to depend on that. This was again confirmed in a way by (\cite{liu2021}), where they found utterances with omissions are more undesirable and affected the performance of the dependency parser. Therefore when it comes to preprocessing data like CHILDES corpora, it depends on the actual intended use case.

\section{Extracting Information from Dependent Tiers}

\paragraph{lemma} After looking at the syntax of the \%mor tier, one should realise that getting the lemma out of a MOR string is not an easy task. There are several pitfalls to avoid:\\
\begin{itemize}
    \item The semantics of the symbols may be ambiguous. Take the dash symbol as an example, for dashed words (words with dash symbols) like \texttt{tic-tac-toe}, it is merely a connector between parts of the word, but it could also appear as an indicator for suffixes, as in the example of \texttt{part|get-PRESP} for the word \emph{getting}.
    \item One should also notice that the stem is not the lemma, just that for English, these two share the same form in most cases. For instance, for \emph{untied}, we have the MOR segment \texttt{un\#v|tie-PASTP}, which means in plain English that the word has a stem \emph{tie} with prefix \emph{un}, its part-of-speech is verb and this word form is the past participle of the verb. On the other hand, in CoNLL-U, the lemma field is reserved for lemma, not word stem, therefore the prefix should be put back in place to produce the real lemma.
\end{itemize}


\texttt{prefix\#pos|stem\&fusionalsuffix-suffix=translation}

% \texttt{compound_pos|+component_pos|component_stem+component_pos|component_stem}

\texttt{prefix\#pos|stem\&fusionalsuffix-suffix=translation}

\lstset{
numbers = none,
frame = single,
}

\begin{lstlisting}[caption={Excerpt from Adam/040217.cha of the Brown Corpus (\cite{brown1973})}, label={lst:chatsent3}]
*CHI:   dat's [: that's] hard .
%mor:   pro:dem|that~cop|be&3S adv|hard .
%gra:   1|2|SUBJ 2|0|ROOT 3|2|JCT 4|2|PUNCT
\end{lstlisting}

Listing \ref{lst:chatsent3} is an example of an utterance with dependent tiers.\\
