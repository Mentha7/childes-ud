\chapter{Implementation of chatconllu} % Main chapter title

\label{Chapter3} % For referencing the chapter elsewhere, use \ref{Chapter3}

% To make the tool manageable within the frame of a bachelor's thesis, chatconllu is designed based on several selected corpora from the CHILDES database.
\section{Data Structures}
From \Cref{Chapter2} we know that CoNLL-U files are collections of sentences, where each sentence has comment lines and token lines. With \Cref{tab:info}, I also showed that to keep as much information from CHAT files as possible, information exclusively relevant to CHAT had to be stored in sentence comments.

chatconllu took the idea from the CoNLL-U sentence and propose to use two object classes---\texttt{Sentence} and \texttt{Token}---to organise the diverse types of information in CHAT files. The benefit of using these two uncomplicated structures is that they correspond well to the CoNLL-U way of information organisation, making writing to CoNLL-U strings straightforward.

\paragraph{\texttt{Sentence}}

Each \texttt{Sentence} object is comprised of a list of \texttt{Token} objects and several additional attributes to store sentence-level information like dependent tiers and file-level meta data like headers preceding the current utterance. The list of \texttt{Token} objects corresponds to the words in the normalised utterance. A normalised utterance is an utterance with the CHAT transcription codes removed. When a \texttt{MOR} tier is present, the tokens should have a one-to-one correspondence with the strings in the \texttt{MOR} tier.

\paragraph{\texttt{Token}}

The attributes of the \texttt{Token} object include the \textlf{10} fields of a normal CoNLL-U token, with the addition of three attributes:
\begin{itemize}
    \item \texttt{multi}---to store the end index of a multi-word token.
    \item \texttt{type}---to store the clitics type
    \item \texttt{surface}---to store the special form of a word (\textcolor{orange}{note-to-self: probably obsolete})
\end{itemize}

Since \texttt{Token} is organised parallel to the UD-style token, it can be directly converted to a token line \sidenote{Or lines for multi-word tokens}.

\section{Overview of chatconllu}

The behaviour of chatconllu can be described in plain English: For all files or files with name specifications in the given directory, depending on the input file format, convert them to the other format and save the output as different files. I used the \emph{pyconll} \sidenote{pyconll is a low-level API for processing CoNLL-U formatted files.} package developed by (\cite{pyconll}) for parsing the converted CoNLL-U files and accessing the values of token fields. I chose to use it instead of writing my own script because I want to quickly validate the general structure of the converted files. By using the load method from a distributed package, I can get instant feedback if the file is malformed. chatconllu uses argparse to process inputs from the command line. With optional arguments, it's also possible to finetune the desired output by asking chatconllu to generate new dependent tiers, or to omit existing dependent tiers during format conversion. All commands of chatconllu can be found via the \texttt{chatconllu --help} command after proper installation. The optional commands are listed in the Appendix\todo{list optional commands in the Appendix}.

\section{CHAT to CoNLL-U}

The conversion from CHAT to CoNLL-U is outlined as follows:
\begin{enumerate}
    \item parse the CHAT file to obtain utterances grouped with dependent tiers
    \item clean the utterance by removing CHAT transcription codes
    \item extract token information from the utterance-tiers group to construct \texttt{Token} objects
    \item construct \texttt{Sentence} object with the created \texttt{Token} objects and other utterance-level information
    \item write the sentences in CoNLL-U format to an output file
\end{enumerate}

\subsection{Normalising Utterances}

To clean the utterances, I wrote a helper method \texttt{normalise\_utterances}. This method is a simple parser by itself. The input of this method is an utterance with CHAT transcription codes. The output is a tuple where the first value is a list of tokens in the utterance. After the removal of transcription codes, if the \texttt{\%mor} is given, the length of the list should match the number of strings in the \texttt{\%mor} tier. The second value is simply the line itself, which is to be stored as is as a sentence comment  in the output file, allowing for the recovery of the original utterance in the back-conversion process.

\paragraph{regular expressions} To remove the CHAT transcription codes, one has to first identify them in the utterance string. Regular expressions are designed for the codes described in the CHAT manual and are tested on the chosen corpora. It is worth noticing that although some regular expressions can be merged into one, they are deliberately left not to, to keep the them manageable for future human inspection, addition, editing, and case-wise debugging.

\paragraph{nested angle brackets} The original algorithm I used was simply parsing the string by incrementing the index of current character and matching the remaining line with defined regular expressions. It was successful in processing flat structures, but not nested structures. Although nested structures are argurably rare in the corpora and the max level of nesting is just one, I still wanted a better solution to the problem.

In CHAT, nested structures occur with \emph{scoped symbols}. Scoped symbols refer to segments of speech comprising of more than one word. The tokens in the segment are enclosed in angle brackets, while the markers which describe the segment are enclosed in square brackets that follows. There can be more than one square-bracket-enclosed descriptors following a angle-bracketed speech segment, but no other materials are allowed between the scope enclosed in angle brackets and the symbols enclosed in square brackets.

The challenge of nested angle brackets can be illustrated using the following examples:
% example from Brown/Sarah/21024, sentence 733

\texttt{<<yeah yeah> [/] yeah> [?] .}\todo{add example and pseudo code for parsing nested brackets, cite answer on stackoverflow}

\paragraph{to normalise or not} Although chatconllu can clean the CHAT annotation codes relatively well, it remains a question whether one really wants to obtain the \emph{clean} version of the utterance. On the one hand, word-based information in \texttt{\%mor} and \texttt{\%gra} tiers is only associated with tokens in the cleaned utterance. Therefore, to use the information provided in these tiers, one has to remove unannotated words (or nonwords). On the other hand, removing things like speech errors makes spontaneous conversational data less speech-like. Or worse, it could make the real sentence structure of the sometimes already ungrammatical utterance even less obvious, at least for dependency parsers. In his study of creating a dependency treebank using Dutch CHILDES corpora, during the process of cleaning the encoded utterances, (\cite{odijk2018anncor}) observed that blindly removing unintelligible words results in a decrease in the performance of the dependency parser they used. For example, although \texttt{xxx} is a nonword, it still functions as a placeholder \sidenote{Think of it as 0 in digits or as a masked token.} in the sentence \todo{add example sentence}(\textcolor{orange}{example sentence}). The Alpino parser (\textcolor{orange}{citation needed}) \todo{add citation for Alpino parser} they used seems to depend on the positional information, despite the word is unknown. This was again confirmed in a way by (\cite{liu2021}), where they found utterances with omissions are more undesirable in dependency parsing and affected the performance of the dependency parser. Therefore when it comes to preprocessing spoken data like the CHILDES corpora. The degree of normalisation depends on the intended use case.

\subsection{Extracting Information from the MOR tier}

\lstset{
numbers = none,
frame = single,
}

\begin{lstlisting}[caption={Example to show the different number of strings in \texttt{\%mor} and \texttt{\&gra} tiers due to contracted form \emph{that's}.}, label={lst:chatsent3}]
*CHI:   dat's [: that's] hard .
%mor:   pro:dem|that~cop|be&3S adv|hard .
%gra:   1|2|SUBJ 2|0|ROOT 3|2|JCT 4|2|PUNCT
\end{lstlisting}


Since morphosyntactic information in \texttt{\%mor} and \texttt{\%gra} is kept with individual tokens in CoNLL-U files, they have to be processed in such a way that all relevant information from both tiers (if present) are associated with the respective token in the utterance. It can be more complicated with clitics and contractions, which results in different lengths of the \texttt{\%mor} and \texttt{\%gra} tiers, shown in Listing \ref{lst:chatsent3} \sidenote{from the same file in the Brown Corpus as Listing\ref{lst:chatsent1}}. Clitics, like the English contracted forms with auxiliary verbs \emph{I've}, the English possessive \emph{Mercury's}, and contractions, like the English negation particle \emph{don't} or the French \emph{articles compos√©s}--\emph{du} \sidenote{"du" is the contraction of "de" and "le"}, are treated by the \texttt{MOR} grammar as one word group and assigned a single string with MOR codes, while \texttt{GRASP} gives a separate string representation for each of its parts.\\


\paragraph{MOR syntax} \todo{describe MOR syntax}
Example:\\
\texttt{prefix\#pos|stem\&fusionalsuffix-suffix=translation}
% info to get from the %mor tier: pos, lemma, feats and things to be stored in MISC
% ref tab:info or partially reproduce it here


\paragraph{lemma} After looking at the syntax of the \%mor tier, one should realise that getting the lemma out of a MOR string is not as straightforward as it seems. There are several pitfalls to avoid:\\
\begin{itemize}
    \item The semantics of the symbols may be ambiguous. Take the dash symbol \texttt{-} as an example, for words like \texttt{tic-tac-toe}, the \texttt{-} merely connect parts of the word, but in the example of \texttt{part|get-PRESP} for the word \emph{getting}, \texttt{-} could be an indicator for the present participle suffix \texttt{-ing}.
    \item One should also notice that the stem is not always the lemma, just that for English, these two share the same form in most cases. For instance, for \emph{untied}, we have the MOR segment \texttt{un\#v|tie-PASTP}, which means in plain English that the word has a stem \emph{tie} with prefix \emph{un}, its part-of-speech is verb and this word form is the past participle of the verb. Since in CoNLL-U, the \texttt{LEMMA} field is reserved for lemma and not word stem, the prefix should be put back in place to produce the real lemma.
    \item Compound strings are structured differently and cannot be parsed in the same way. For word like \texttt{n|+n|peanut+n|butter}, instead of parsing this string using a different method, chatconllu simply use the word form as lemma. In this example, the word form and the lemma should both be \texttt{peanutbutter}.
\end{itemize}

\paragraph{misc}
The extensibility of CHAT enables transcribers to note down almost any information they want, but the CoNLL-U format is focused on selective grammatical information of the tokens in the sentences and only values defined either universally or language-specifically by UD are accepted. To protect against the loss of information during format conversion, chatconllu uses the \texttt{MISC} field to store information that is outside of the previous nine fields.
One should also take care not to use reserved symbols like the field-value separator \texttt{|}. For instance, in one wants to save the compound components \texttt{+n|peanut+n|butter} for \texttt{n|+n|peanut+n|butter} as is in , the \texttt{|} character should first be replaced with semantically insignificant symbols like \texttt{@}, and only to be replaced back to \texttt{|} after the \texttt{MISC} field is processed as a dictionary during the reconstruction of dependent tiers. \todo{table for all keys used in the MISC field; table for reserved symbols in CHAT and CoNLL-U.}
% avoid conflicts replace separator

% \texttt{compound_pos|+component_pos|component_stem+component_pos|component_stem}

\subsection{Extracting Information from the GRA tier}

\paragraph{GRA syntax}
Example:


\subsection{Sentence-level Information}
Sentence-level info can also be stored in the comments in CoNLL-U. Meta-information given in the headers of CHAT files are saved as file-initial comments before the first sentence because they are file-specific.
However, one has to admit that storing information like this reduces readability. Another thing is that information stored in this way are susceptible to changes accidentally made which cannot be detected by format validators. One possible solution is to validate the respective format before and after conversion is performed.


\section{CoNLL-U to CHAT}

The conversion from CoNLL-U back to CHAT is described in the following steps:
\begin{enumerate}
    \item load the CoNLL-U file with pyconll to obtain sentences
    \item clean the utterance by removing CHAT speech codes
    \item extract token information from the groups to construct \texttt{Token} objects
    \item construct \texttt{Sentence} object with the created tokens
    \item write sentence in CoNLL-U format to output file
\end{enumerate}

\subsection{Reconstruct Dependent Tiers}

\subsection{Generating New Tiers}
